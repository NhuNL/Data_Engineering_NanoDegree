{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Create the data pipelines and design ETL to run data to data model using immigration data and other available sources.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary packages and libraries for the project\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf, col, to_timestamp, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "\n",
    "from us_state_abbrev import state_udf, abbrev_state, abbrev_state_udf,city_code_udf,city_codes\n",
    "from immigration_codes import country_udf\n",
    "\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope\n",
    "Based on I94 Immigration Data and some other sources, creating a data model to see information about immigration movement e.g which city most immigration arrive to USA, most common visa type of immigration. \n",
    "\n",
    "\n",
    "#### Describe and Gather Data  \n",
    "\n",
    "Data Sources:\n",
    "- I94 Immigration Data: This data comes from the US National Tourism and Trade Office (https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "- World Temperature Data: https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data\n",
    "- U.S. City Demographic Data:https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/\n",
    "- Airport Code Table https://datahub.io/core/airport-codes#data\n",
    "\n",
    "Tools: Jupyter notebook, AWS S3 Buckets, AWS Redshift, SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t # Create Spark session\n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "        config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .enableHiveSupport().getOrCreate()\n",
    "\n",
    "#Build SQL context object\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "\n",
    "# read immigration data\n",
    "df_immigration=spark.read.format('com.github.saurfang.sas.spark').load(\"../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\")\n",
    "\n",
    "demo_graphics=spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")\n",
    "\n",
    "airport=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"airport-codes_csv.csv\")\n",
    "\n",
    "temperatureData=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"GlobalLandTemperaturesByState.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|  4.0|2016.0|   6.0| 135.0| 135.0|    XXX|20612.0|   null|   null|   null|  59.0|    2.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null|    null| null|      Z|   null|      U|   null| 1957.0|10032016|  null|  null|   null|1.4938462027E10| null|      WT|\n",
      "|  5.0|2016.0|   6.0| 135.0| 135.0|    XXX|20612.0|   null|   null|   null|  50.0|    2.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null|    null| null|      Z|   null|      U|   null| 1966.0|10032016|  null|  null|   null|1.7460063727E10| null|      WT|\n",
      "|  6.0|2016.0|   6.0| 213.0| 213.0|    XXX|20609.0|   null|   null|   null|  27.0|    3.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null|    null| null|      T|   null|      U|   null| 1989.0|     D/S|  null|  null|   null|  1.679297785E9| null|      F1|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view top 5 rows of immigration data\n",
    "df_immigration.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "- Filter out null values from immigration data set df_immigration, get origin country from country_udf by converting i94res codes to country.\n",
    "- Drop duplicates from df_immigration and temparature data.\n",
    "\n",
    "- Filter average temperature data only for the United States and only year = 2013 and create new fields with year, month, fahrenheit and run abbreviations function.\n",
    "\n",
    "- Sort city demographic data then calculate percentages and select percentages fields and drop duplicates.\n",
    "- Filter airport data for \"small_airport\" and use substring to return the state code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Immigration Data (number of entry by state and origin country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter nulls from data set df_spark\n",
    "# using all mapping tables from the udf to convert i94res codes to country name\n",
    "# filter out NULLS and run country_udf function to show state names\n",
    "# country_udf, abbrev_state_udf and city_code_udf were created with data from i94 SAS labels Descriptions file.\n",
    "\n",
    "i94_data=df_immigration.filter(df_immigration.i94addr.isNotNull())\\\n",
    ".filter(df_immigration.i94res.isNotNull())\\\n",
    ".filter(col(\"i94addr\").isin(list(abbrev_state.keys())))\\\n",
    ".filter(col(\"i94port\").isin(list(city_codes.keys())))\\\n",
    ".withColumn(\"origin_country\",country_udf(df_immigration[\"i94res\"]))\\\n",
    ".withColumn(\"dest_state_name\",abbrev_state_udf(df_immigration[\"i94addr\"]))\\\n",
    ".withColumn(\"i94yr\",col(\"i94yr\").cast(\"integer\"))\\\n",
    ".withColumn(\"i94mon\",col(\"i94mon\").cast(\"integer\"))\\\n",
    ".withColumn(\"city_port_name\",city_code_udf(df_immigration[\"i94port\"]))\n",
    "\n",
    "# Create new dataset to view immigration data by state and origin country\n",
    "\n",
    "I94_Data_by_state_origin_country = i94_data.select(\"cicid\",col(\"i94yr\").alias(\"year\"),col(\"i94mon\").alias(\"month\"),\\\n",
    "                             \"origin_country\",\"i94port\",\"city_port_name\",col(\"i94addr\").alias(\"state_code\"),\"dest_state_name\",\\\n",
    "                                                  col(\"count\").alias(\"number of entry\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+---------------+\n",
      "|cicid|year|month|origin_country|i94port|    city_port_name|state_code|dest_state_name|number of entry|\n",
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+---------------+\n",
      "| 41.0|2016|    6|   SOUTH KOREA|    SFR|SAN FRANCISCO     |        CA|     California|            1.0|\n",
      "| 42.0|2016|    6|   SOUTH KOREA|    SFR|SAN FRANCISCO     |        CA|     California|            1.0|\n",
      "| 45.0|2016|    6|       ROMANIA|    HOU|HOUSTON           |        TX|          Texas|            1.0|\n",
      "| 52.0|2016|    6|       ALBANIA|    BOS|BOSTON            |        MA|  Massachusetts|            1.0|\n",
      "| 53.0|2016|    6|       ALBANIA|    NEW|NEWARK/TETERBORO  |        PA|   Pennsylvania|            1.0|\n",
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view first 5 records of the new data frame of immigration data\n",
    "I94_Data_by_state_origin_country.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the GlobalLandTemparature filter data 2013 for US only. \n",
    "# drop duplicates\n",
    "# add column average temparature in celcius by converting avg_temp_fahrenheit\n",
    "# as the sample data set contains data up to 2013 only, so we will filter only for 2013\n",
    "\n",
    "Temperatures=temperatureData.filter(temperatureData[\"country\"]==\"United States\")\\\n",
    ".filter(year(temperatureData[\"dt\"])==2013)\\\n",
    ".withColumn(\"year\",year(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"month\",month(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"avg_temp_fahrenheit\",temperatureData[\"AverageTemperature\"]*9/5+32)\\\n",
    ".withColumn(\"state_abbrev\",state_udf(temperatureData[\"State\"]))\n",
    "\n",
    "Temperatures=Temperatures.select(\"year\",\"month\",round(col(\"AverageTemperature\"),1).alias(\"avg_temp_celcius\"),\\\n",
    "                                       round(col(\"avg_temp_fahrenheit\"),1).alias(\"avg_temp_fahrenheit\"),\n",
    "                                       \"state_abbrev\",\"State\",\"Country\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "|year|month|avg_temp_celcius|avg_temp_fahrenheit|state_abbrev|        State|      Country|\n",
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "|2013|    7|            23.4|               74.1|          MA|Massachusetts|United States|\n",
      "|2013|    3|            -1.9|               28.5|          SD| South Dakota|United States|\n",
      "|2013|    9|            14.1|               57.4|          ME|        Maine|United States|\n",
      "|2013|    1|            -1.3|               29.7|          PA| Pennsylvania|United States|\n",
      "|2013|    9|            25.1|               77.2|          AL|      Alabama|United States|\n",
      "+----+-----+----------------+-------------------+------------+-------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Temperatures.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U.S. Demographic Data by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create new columns for percentages of other dimension (for example: Male Population, Number of Veterans etc.)\n",
    "# by population\n",
    "\n",
    "demo_graphics = demo_graphics\\\n",
    ".withColumn(\"Median Age\",col(\"Median Age\").cast(\"float\"))\\\n",
    ".withColumn(\"pct_male_pop\",demo_graphics[\"Male Population\"]/demo_graphics[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_female_pop\",demo_graphics[\"Female Population\"]/demo_graphics[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_veterans\",demo_graphics[\"Number of Veterans\"]/demo_graphics[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_foreign_born\",demo_graphics[\"Foreign-born\"]/demo_graphics[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_race\",demo_graphics[\"Count\"]/demo_graphics[\"Total Population\"]*100)\\\n",
    ".orderBy(\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select states and calculated columns from above to view demo graphics data by states\n",
    "\n",
    "demo_graphics_by_state=demo_graphics.select(\"State\",col(\"State Code\").alias(\"state_code\"),\\\n",
    "                                 col(\"Median Age\").alias(\"median_age\"),\\\n",
    "                                 \"pct_male_pop\",\\\n",
    "                                 \"pct_female_pop\",\\\n",
    "                                 \"pct_veterans\",\\\n",
    "                                 \"pct_foreign_born\",\\\n",
    "                                 \"Race\",\\\n",
    "                                 \"pct_race\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+------------------+------------------+-----------------+------------------+--------------------+------------------+\n",
      "|  State|state_code|median_age|      pct_male_pop|    pct_female_pop|     pct_veterans|  pct_foreign_born|                Race|          pct_race|\n",
      "+-------+----------+----------+------------------+------------------+-----------------+------------------+--------------------+------------------+\n",
      "|Alabama|        AL|      38.1| 48.52311304292649| 51.47688695707351|8.797339171081992| 6.710767050562094|Black or African-...|32.552322937487446|\n",
      "|Alabama|        AL|      29.1| 48.09229392503407| 51.90770607496593|3.708637556183774| 4.785535601700258|  Hispanic or Latino| 2.516829709776485|\n",
      "|Alabama|        AL|      38.9|47.636815920398014|52.363184079601986|9.378701729447998| 2.515695332859512|               Asian|1.7398128405591091|\n",
      "|Alabama|        AL|      35.4| 47.15284217243477| 52.84715782756524|7.455654931052018|4.6548612565184015|               White|36.665071340970954|\n",
      "|Alabama|        AL|      38.9|47.636815920398014|52.363184079601986|9.378701729447998| 2.515695332859512|  Hispanic or Latino| 2.523098791755508|\n",
      "+-------+----------+----------+------------------+------------------+-----------------+------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show demo graphics data by state\n",
    "demo_graphics_by_state.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pivot table for avg percentage of race by state, median_age, male population etc.\n",
    "pivot_demo_graphics_by_state = demo_graphics_by_state.groupBy(\"State\",\"state_code\",\"median_age\",\"pct_male_pop\",\\\n",
    "                                    \"pct_female_pop\",\"pct_veterans\",\\\n",
    "                                    \"pct_foreign_born\").pivot(\"Race\").avg(\"pct_race\")\n",
    "\n",
    "# rename header for race\n",
    "pivot_demo_graphics_by_state=pivot_demo_graphics_by_state.select(\"State\",\"state_code\",\"median_age\",\"pct_male_pop\",\"pct_female_pop\",\"pct_veterans\",\"pct_foreign_born\",\\\n",
    "                                         col(\"American Indian and Alaska Native\").alias(\"native_american\"),\\\n",
    "                                         col(\"Asian\"),col(\"Black or African-American\").alias(\"Black\"),\\\n",
    "                                         col(\"Hispanic or Latino\").alias(\"hispanic_or_latino\"),\"White\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the average of each column per state. \n",
    "\n",
    "pivot = pivot_demo_graphics_by_state.groupBy(\"State\",\"state_code\").avg(\"median_age\",\"pct_male_pop\",\"pct_female_pop\",\\\n",
    "                                                       \"pct_veterans\",\"pct_foreign_born\",\"native_american\",\\\n",
    "                                                       \"Asian\",\"Black\",\"hispanic_or_latino\",\"White\").orderBy(\"State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding the percentages and rename columns\n",
    "results =pivot.select(\"State\",\"state_code\",round(col(\"avg(median_age)\"),1).alias(\"median_age\"),\\\n",
    "                  round(col(\"avg(pct_male_pop)\"),1).alias(\"pct_male_pop\"),\\\n",
    "                   round(col(\"avg(pct_female_pop)\"),1).alias(\"pct_female_pop\"),\\\n",
    "                   round(col(\"avg(pct_veterans)\"),1).alias(\"pct_veterans\"),\\\n",
    "                   round(col(\"avg(pct_foreign_born)\"),1).alias(\"pct_foreign_born\"),\\\n",
    "                   round(col(\"avg(native_american)\"),1).alias(\"native_american\"),\\\n",
    "                   round(col(\"avg(Asian)\"),1).alias(\"Asian\"),\\\n",
    "                   round(col(\"avg(hispanic_or_latino)\"),1).alias(\"hispanic_or_latino\"),\\\n",
    "                   round(col(\"avg(Black)\"),1).alias(\"Black\"),\\\n",
    "                   round(col('avg(White)'),1).alias('White')\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|     State|state_code|median_age|pct_male_pop|pct_female_pop|pct_veterans|pct_foreign_born|native_american|Asian|hispanic_or_latino|Black|White|\n",
      "+----------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|   Alabama|        AL|      36.2|        47.2|          52.8|         6.8|             5.1|            0.8|  2.9|               3.6| 45.0| 52.0|\n",
      "|    Alaska|        AK|      32.2|        51.2|          48.8|         9.2|            11.1|           12.2| 12.3|               9.1|  7.7| 71.2|\n",
      "|   Arizona|        AZ|      35.0|        48.8|          51.2|         6.6|            12.6|            2.8|  5.1|              28.8|  6.0| 82.7|\n",
      "|  Arkansas|        AR|      32.8|        48.4|          51.6|         5.2|            10.7|            1.8|  4.1|              14.2| 21.8| 68.0|\n",
      "|California|        CA|      36.2|        49.4|          50.6|         4.1|            27.6|            1.7| 17.9|              37.8|  7.5| 62.7|\n",
      "+----------+----------+----------+------------+--------------+------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the results\n",
    "results.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U.S. Airport Data by State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the airport-codes data, filter only small airport\n",
    "\n",
    "airport_data=airport.filter(airport[\"type\"]==\"small_airport\")\\\n",
    ".filter(airport[\"iso_country\"]==\"US\")\\\n",
    ".withColumn(\"iso_region\",substring(airport[\"iso_region\"],4,2))\\\n",
    ".withColumn(\"elevation_ft\",col(\"elevation_ft\").cast(\"float\"))\n",
    "\n",
    "# Calculate average elevation_ft per state by group by airport data frame above.\n",
    "airport_by_elevation_ft=airport_data.groupBy(\"iso_country\",\"iso_region\").avg(\"elevation_ft\")\n",
    "\n",
    "# Select relevant columns and drop duplicates\n",
    "airport_avg_elevation_ft=airport_by_elevation_ft.select(col(\"iso_country\").alias(\"country\"),\\\n",
    "                                               col(\"iso_region\").alias(\"state\"),\\\n",
    "                                               round(col(\"avg(elevation_ft)\"),1).alias(\"avg_elevation_ft\")).orderBy(\"iso_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----------------+\n",
      "|country|state|avg_elevation_ft|\n",
      "+-------+-----+----------------+\n",
      "|     US|   AK|           545.1|\n",
      "|     US|   AL|           414.6|\n",
      "|     US|   AR|           488.4|\n",
      "|     US|   AZ|          3098.0|\n",
      "|     US|   CA|          1261.4|\n",
      "+-------+-----+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_avg_elevation_ft.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Star Schema\n",
    "#### 1/ Dimension Tables\n",
    "##### airport_table\n",
    "- country, state, avg_elevation_ft\n",
    "\n",
    "##### city_stats\n",
    "- State,state_code,median_age,pct_male_pop,pct_female_pop,\n",
    "pct_veterans,pct_foreign_born,native_american,Asian,hispan- ic_or_latino,Black,White\n",
    "\n",
    "##### immigration_table\n",
    "- cicid,year,month,origin_country,i94port,city_port_us,state,dest_state_us\n",
    "\n",
    "##### avg_state_temps\n",
    "- year,month,AverageTemperature,avg_temp_fahrenheit,state_abbrev,State,Country\n",
    "\n",
    "#### 2/ Fact Table\n",
    "##### immigration_fact_table\n",
    "- year, immig_month, immig_origin, immig_state, 'to_immig_state_count', 'avg_temp_fahrenheit', 'avg_elevation_ft', 'pct_foreign_born', 'native_american', 'Asian', 'hispanic_or_latino', 'Black', 'White'\n",
    "\n",
    "Star scheme was chosen because it's simple and suitable for data analysis later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- From the clean dataset above, create 4 dimensions tables (as in schema)\n",
    "\n",
    "2- Fact table is created as a SQL query with joins to dimension tables.\n",
    "\n",
    "3- Fact table is converted back to a spark dataframe.\n",
    "\n",
    "4- Fact table is written as final parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "\n",
    "# Dimension tables are created from clean data set above.\n",
    "\n",
    "I94_Data_by_state_origin_country.createOrReplaceTempView(\"immigration\")\n",
    "results.createOrReplaceTempView(\"demographics\")\n",
    "airport_avg_elevation_ft.createOrReplaceTempView(\"airport\")\n",
    "Temperatures.createOrReplaceTempView(\"temperature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set threshold time to zero for SQL joining and parquet writing the files, \n",
    "# so it is allowed to write without limitation for loading data\n",
    "\n",
    "sqlContext.setConf(\"spark.sql.autoBroadcastJoinThreshold\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this query to count number of immigrants to the USA by year, month, states and other dimensions\n",
    "# This is one of the example that we could use to get the result from tables above.\n",
    "# The result of this query becomes our fact table now.\n",
    "\n",
    "immigration_result=spark.sql(\"\"\"SELECT \n",
    "                                    m.year,\n",
    "                                    m.month AS immig_month,\n",
    "                                    m.origin_country AS immig_origin,\n",
    "                                    m.dest_state_name AS to_immig_state,\n",
    "                                    COUNT(m.state_code) AS to_immig_state_count,\n",
    "                                    t.avg_temp_fahrenheit,\n",
    "                                    a.avg_elevation_ft,\n",
    "                                    d.pct_foreign_born,\n",
    "                                    d.native_american,\n",
    "                                    d.Asian,\n",
    "                                    d.hispanic_or_latino,\n",
    "                                    d.Black,\n",
    "                                    d.White\n",
    "                                    \n",
    "                                    FROM immigration m JOIN temperature t ON m.state_code=t.state_abbrev AND m.month=t.month\n",
    "                                    JOIN demographics d ON d.state_code=t.state_abbrev\n",
    "                                    JOIN airport a ON a.state=t.state_abbrev\n",
    "                                    \n",
    "                                    GROUP BY m.year,m.month, m.origin_country,\\\n",
    "                                    m.dest_state_name,m.state_code,t.avg_temp_fahrenheit,a.avg_elevation_ft,\\\n",
    "                                    d.pct_foreign_born,d.native_american,\\\n",
    "                                    d.Asian,d.hispanic_or_latino,\\\n",
    "                                    d.hispanic_or_latino,d.White,\\\n",
    "                                    d.Black\n",
    "                                    \n",
    "                                    ORDER BY m.origin_country,m.state_code\n",
    "                                    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|year|immig_month|immig_origin|to_immig_state|to_immig_state_count|avg_temp_fahrenheit|avg_elevation_ft|pct_foreign_born|native_american|Asian|hispanic_or_latino|Black|White|\n",
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "|2016|          6| AFGHANISTAN|      Arkansas|                   1|               77.7|           488.4|            10.7|            1.8|  4.1|              14.2| 21.8| 68.0|\n",
      "|2016|          6| AFGHANISTAN|       Arizona|                   1|               79.9|          3098.0|            12.6|            2.8|  5.1|              28.8|  6.0| 82.7|\n",
      "|2016|          6| AFGHANISTAN|    California|                  31|               72.5|          1261.4|            27.6|            1.7| 17.9|              37.8|  7.5| 62.7|\n",
      "|2016|          6| AFGHANISTAN|      Colorado|                   1|               65.9|          5912.8|             9.6|            2.0|  4.9|              22.2|  4.2| 88.0|\n",
      "|2016|          6| AFGHANISTAN|   Connecticut|                   4|               67.3|           490.3|            25.2|            1.3|  5.3|              34.8| 24.3| 59.6|\n",
      "+----+-----------+------------+--------------+--------------------+-------------------+----------------+----------------+---------------+-----+------------------+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_result.toDF('year', 'immig_month', 'immig_origin', 'to_immig_state', \\\n",
    "          'to_immig_state_count', 'avg_temp_fahrenheit', 'avg_elevation_ft',\\\n",
    "          'pct_foreign_born', 'native_american', 'Asian', 'hispanic_or_latino', 'Black', 'White').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final step is to write the fact table to parquet \n",
    "\n",
    "immigration_result.write.parquet(\"immigration_result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|No_of_records_Immigration|\n",
      "+-------------------------+\n",
      "|                  3214208|\n",
      "+-------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|No_of_records_demographics|\n",
      "+--------------------------+\n",
      "|                        49|\n",
      "+--------------------------+\n",
      "\n",
      "+---------------------+\n",
      "|No_of_records_airport|\n",
      "+---------------------+\n",
      "|                   51|\n",
      "+---------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|No_of_records_temperature|\n",
      "+-------------------------+\n",
      "|                      459|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "# Check numbers of records of 4 dimension tables\n",
    "\n",
    "spark.sql('SELECT COUNT(*) as No_of_records_Immigration FROM immigration').show()\n",
    "spark.sql('SELECT COUNT(*) as No_of_records_demographics FROM demographics').show()\n",
    "spark.sql('SELECT COUNT(*) as No_of_records_airport FROM airport').show()\n",
    "spark.sql('SELECT COUNT(*) as No_of_records_temperature FROM temperature').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+\n",
      "| year|month|country|state|\n",
      "+-----+-----+-------+-----+\n",
      "|false|false|  false|false|\n",
      "+-----+-----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This select can check whether any null values in year, month, origin, state from the fact table\n",
    "# Data came through fine, if result is false.\n",
    "immigration_result.select(isnull('year').alias('year'),\\\n",
    "                             isnull('immig_month').alias('month'),\\\n",
    "                             isnull('immig_origin').alias('country'),\\\n",
    "                             isnull('to_immig_state').alias('state')).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|Total_No_of_Immigrants|\n",
      "+----------------------+\n",
      "|               3207230|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For example one of the checks, we can try is getting total number of immigrants\n",
    "\n",
    "immigration_result.select(sum('to_immig_state_count').alias('Total_No_of_Immigrants')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "DataDictionary is in a separate file (DataDictionary.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1/ Tools and technologies for the project\n",
    "- For ETL and analysis using Apache Spark >> because the dataset is relative small (3.2 millions). \n",
    "- Jupyter notebook for running main scripts, as joining tables for analysis can be quickly demonstrated within the notebook, as well as for data quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/ Data should be updated monthly or quarterly. The result of the data can be created monthly and quarterly reports for the government.\n",
    "- Some visualisation can be quickly created in jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/ Under the following scenarios, I would approach the problem differently:\n",
    "- The data was increased by 100x, we still could use Apache Spark for ETL. Data will be stored in AWS. Imcremental loads could be used to shorten the time for ETL. \n",
    "\n",
    "- Update data on a daily basis by 7am every day. >> Airflow can be used for ETL. We could create schedules for daily data updates. If some visualization on a dashboard is requires, we could the result table can be stored in AWS Redshift, visualisation with facts can be done quickly in Jupyter notebook (using matplotlib, pandas, seaborn etc..)\n",
    "\n",
    "- This way data needs to be accessed by 100+ people. All we need to do is giving users theirs credentials to AWS Redshift. So they can create their own data analysis. If a standardised or centralised reporting is requires, we could use Tableau, Qlik, Looker for data analysis, visualisation. Those BI tools can connect to AWS Redshift. As soon as the data is updated in AWS Redshif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
